{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d04597d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found classes: ['Astrocitoma', 'Carcinoma', 'Ependimoma', 'Ganglioglioma', 'Germinoma', 'Glioblastoma', 'Granuloma', 'Meduloblastoma', 'Meningioma', 'NORMAL', 'Neurocitoma', 'Oligodendroglioma', 'Papiloma', 'Schwannoma', 'Tuberculoma']\n",
      "Model for Astrocitoma already exists. Skipping...\n",
      "Model for Carcinoma already exists. Skipping...\n",
      "Model for Ependimoma already exists. Skipping...\n",
      "Model for Ganglioglioma already exists. Skipping...\n",
      "Model for Germinoma already exists. Skipping...\n",
      "Model for Glioblastoma already exists. Skipping...\n",
      "Model for Granuloma already exists. Skipping...\n",
      "Model for Meduloblastoma already exists. Skipping...\n",
      "Model for Meningioma already exists. Skipping...\n",
      "Model for NORMAL already exists. Skipping...\n",
      "Model for Neurocitoma already exists. Skipping...\n",
      "Model for Oligodendroglioma already exists. Skipping...\n",
      "Model for Papiloma already exists. Skipping...\n",
      "Model for Schwannoma already exists. Skipping...\n",
      "Model for Tuberculoma already exists. Skipping...\n",
      "Model for Astrocitoma already exists. Skipping...\n",
      "Model for Carcinoma already exists. Skipping...\n",
      "Model for Ependimoma already exists. Skipping...\n",
      "Model for Ganglioglioma already exists. Skipping...\n",
      "Model for Germinoma already exists. Skipping...\n",
      "Model for Glioblastoma already exists. Skipping...\n",
      "Model for Granuloma already exists. Skipping...\n",
      "Model for Meduloblastoma already exists. Skipping...\n",
      "Model for Meningioma already exists. Skipping...\n",
      "Model for NORMAL already exists. Skipping...\n",
      "Model for Neurocitoma already exists. Skipping...\n",
      "Model for Oligodendroglioma already exists. Skipping...\n",
      "Model for Papiloma already exists. Skipping...\n",
      "Model for Schwannoma already exists. Skipping...\n",
      "Model for Tuberculoma already exists. Skipping...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, WeightedRandomSampler\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, mean_squared_error, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ==== Hyperparameters and Paths ====\n",
    "DATA_DIR = Path(\"./dataset\")\n",
    "MODEL_SAVE_DIR = Path(\"./models_customcnn\")\n",
    "MODEL_SAVE_DIR.mkdir(exist_ok=True)\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 64\n",
    "LR = 5e-4\n",
    "EPOCHS = 10\n",
    "NUM_WORKERS = 0 if os.name == 'nt' else 4\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# ==== Dataset Setup ====\n",
    "dirs = [d.name for d in DATA_DIR.iterdir() if d.is_dir()]\n",
    "class_names = sorted(dirs)\n",
    "num_classes = len(class_names)\n",
    "print(\"Found classes:\", class_names)\n",
    "\n",
    "class BinaryFolderDataset(Dataset):\n",
    "    def __init__(self, root_dir, positive_class, transform=None):\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "        for cls_dir in Path(root_dir).iterdir():\n",
    "            if not cls_dir.is_dir():\n",
    "                continue\n",
    "            label = 1 if cls_dir.name == positive_class else 0\n",
    "            for ext in (\"*.jpg\", \"*.jpeg\", \"*.png\"):\n",
    "                for img_path in cls_dir.glob(ext):\n",
    "                    self.samples.append((img_path, label))\n",
    "        if not self.samples:\n",
    "            raise RuntimeError(f\"No images found for class '{positive_class}' in {root_dir}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "# ==== Transforms ====\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "# ==== Custom Lightweight CNN ====\n",
    "class LightTumorCNN(nn.Module):\n",
    "    def __init__(self, num_classes=1):\n",
    "        super(LightTumorCNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 3, padding=1), nn.BatchNorm2d(16), nn.ReLU(), nn.MaxPool2d(2),  # 112x112\n",
    "            nn.Conv2d(16, 32, 3, padding=1), nn.BatchNorm2d(32), nn.ReLU(), nn.MaxPool2d(2),  # 56x56\n",
    "            nn.Conv2d(32, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2),  # 28x28\n",
    "            nn.Conv2d(64, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(), nn.MaxPool2d(2)  # 14x14\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x  # shape [B, 1]\n",
    "\n",
    "class LightTumorCNN_v2(nn.Module):\n",
    "    def __init__(self, num_classes=1):\n",
    "        super().__init__()\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding=1, stride=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.blocks = nn.Sequential(\n",
    "            DepthwiseSeparableConv(16, 32), nn.MaxPool2d(2),  # 112x112\n",
    "            DepthwiseSeparableConv(32, 64), nn.MaxPool2d(2),  # 56x56\n",
    "            DepthwiseSeparableConv(64, 128), SEBlock(128), nn.MaxPool2d(2),  # 28x28\n",
    "            DepthwiseSeparableConv(128, 128), nn.MaxPool2d(2)  # 14x14\n",
    "        )\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.blocks(x)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "class DepthwiseSeparableConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1, groups=in_channels)\n",
    "        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.depthwise(x)\n",
    "        x = self.pointwise(x)\n",
    "        x = self.bn(x)\n",
    "        return self.relu(x)\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super().__init__()\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channels, channels // reduction),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(channels // reduction, channels),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        se = self.pool(x).view(b, c)\n",
    "        se = self.fc(se).view(b, c, 1, 1)\n",
    "        return x * se\n",
    "    \n",
    "def create_binary_model():\n",
    "    model = LightTumorCNN(num_classes=1)\n",
    "    return model.to(device)\n",
    "def create_binary_model2():\n",
    "    model = LightTumorCNN(num_classes=1)\n",
    "    return model.to(device)\n",
    "\n",
    "# ==== Training/Evaluation Functions ====\n",
    "def train_epoch(model, loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x)\n",
    "        y = y.unsqueeze(1).float()\n",
    "        loss = criterion(out, y)\n",
    "        preds = (torch.sigmoid(out) > 0.5).int()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * x.size(0)\n",
    "        correct += (preds.view(-1) == y.view(-1)).sum().item()\n",
    "        total += x.size(0)\n",
    "    return running_loss / total, correct / total\n",
    "\n",
    "def eval_epoch(model, loader, criterion):\n",
    "    model.eval()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    y_true, y_pred, y_prob = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            out = model(x)\n",
    "            y = y.unsqueeze(1).float()\n",
    "            loss = criterion(out, y)\n",
    "            probs = torch.sigmoid(out)\n",
    "            preds = (probs > 0.5).int()\n",
    "            running_loss += loss.item() * x.size(0)\n",
    "            correct += (preds.view(-1) == y.view(-1)).sum().item()\n",
    "            total += x.size(0)\n",
    "            y_true.append(y.view(-1).cpu())\n",
    "            y_pred.append(preds.view(-1).cpu())\n",
    "            y_prob.append(probs.view(-1).cpu())\n",
    "    y_true = torch.cat(y_true).numpy()\n",
    "    y_pred = torch.cat(y_pred).numpy()\n",
    "    y_prob = torch.cat(y_prob).numpy()\n",
    "    \n",
    "    acc = correct / total\n",
    "    mse = mean_squared_error(y_true, y_prob)\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    errors = np.sum(y_true != y_pred)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    return {\n",
    "        \"loss\": running_loss / total,\n",
    "        \"acc\": acc,\n",
    "        \"mse\": mse,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"errors\": errors,\n",
    "        \"confusion_matrix\": cm,\n",
    "        \"y_true\": y_true,\n",
    "        \"y_pred\": y_pred\n",
    "    }\n",
    "\n",
    "\n",
    "def create_balanced_loader(dataset, indices, batch_size, num_workers):\n",
    "    targets = [dataset.samples[i][1] for i in indices]\n",
    "    class_sample_count = np.bincount(targets)\n",
    "    weight = 1. / class_sample_count\n",
    "    samples_weight = np.array([weight[t] for t in targets])\n",
    "    sampler = WeightedRandomSampler(samples_weight, num_samples=len(samples_weight), replacement=True)\n",
    "    loader = DataLoader(Subset(dataset, indices), batch_size=batch_size, sampler=sampler, num_workers=num_workers)\n",
    "    return loader\n",
    "\n",
    "def save_binary_histories_to_csv(binary_histories, filename='binary_histories_customcnn.csv'):\n",
    "    with open(filename, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Class'] + [f\"Epoch {i+1}\" for i in range(len(next(iter(binary_histories.values()))))])\n",
    "        for cls, history in binary_histories.items():\n",
    "            writer.writerow([cls] + history)\n",
    "\n",
    "# ==== Training Binary Ensemble Models ====\n",
    "ensemble_models = {}\n",
    "binary_histories = {cls: [] for cls in class_names}\n",
    "\n",
    "for cls in class_names:\n",
    "    model_path = MODEL_SAVE_DIR / f\"best_binary_{cls}.pth\"\n",
    "    if model_path.exists():\n",
    "        print(f\"Model for {cls} already exists. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n--- Training {cls} vs Rest ---\")\n",
    "    ds = BinaryFolderDataset(DATA_DIR, cls, transform=transform)\n",
    "    labels = [label for _, label in ds.samples]\n",
    "    \n",
    "    train_idx, temp_idx = train_test_split(range(len(ds)), test_size=0.3, stratify=labels)\n",
    "    val_idx, test_idx = train_test_split(temp_idx, test_size=0.5, stratify=[labels[i] for i in temp_idx])\n",
    "    \n",
    "    train_loader = create_balanced_loader(ds, train_idx, BATCH_SIZE, NUM_WORKERS)\n",
    "    val_loader = DataLoader(Subset(ds, val_idx), batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "    test_loader = DataLoader(Subset(ds, test_idx), batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "    model = create_binary_model()\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "    best_acc = 0.0\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer)\n",
    "        val_metrics = eval_epoch(model, val_loader, criterion)\n",
    "        val_loss = val_metrics[\"loss\"]\n",
    "        val_acc = val_metrics[\"acc\"]\n",
    "        binary_histories[cls].append(val_acc)\n",
    "        save_binary_histories_to_csv(binary_histories)\n",
    "        print(f\"[{cls}] Epoch {epoch}: Val Acc = {val_acc:.4f}\")\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "    ensemble_models[cls] = model\n",
    "    print(f\"\\n--- Evaluation for {cls} ---\")\n",
    "\n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "    train_metrics = eval_epoch(model, train_loader, criterion)\n",
    "    val_metrics = eval_epoch(model, val_loader, criterion)\n",
    "    test_metrics = eval_epoch(model, test_loader, criterion)\n",
    "\n",
    "    def print_metrics(split, metrics):\n",
    "        print(f\"{split} Accuracy: {metrics['acc']:.4f}\")\n",
    "        print(f\"{split} Precision: {metrics['precision']:.4f}\")\n",
    "        print(f\"{split} Recall: {metrics['recall']:.4f}\")\n",
    "        print(f\"{split} F1 Score: {metrics['f1']:.4f}\")\n",
    "        print(f\"{split} MSE: {metrics['mse']:.4f}\")\n",
    "        print(f\"{split} Errors: {metrics['errors']}\")\n",
    "        print(f\"{split} Confusion Matrix:\\n{metrics['confusion_matrix']}\\n\")\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=metrics['confusion_matrix'], display_labels=[\"Other\", cls])\n",
    "        disp.plot(cmap='Blues')\n",
    "        plt.title(f\"{split} Confusion Matrix for {cls}\")\n",
    "        plt.show()\n",
    "\n",
    "    print_metrics(\"Train\", train_metrics)\n",
    "    print_metrics(\"Validation\", val_metrics)\n",
    "    print_metrics(\"Test\", test_metrics)\n",
    "\n",
    "\n",
    "for cls in class_names:\n",
    "    model_path = MODEL_SAVE_DIR / f\"best_binary_{cls}.pth\"\n",
    "    if model_path.exists():\n",
    "        print(f\"Model for {cls} already exists. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n--- Training {cls} vs Rest ---\")\n",
    "    ds = BinaryFolderDataset(DATA_DIR, cls, transform=transform)\n",
    "    labels = [label for _, label in ds.samples]\n",
    "    \n",
    "    train_idx, temp_idx = train_test_split(range(len(ds)), test_size=0.3, stratify=labels)\n",
    "    val_idx, test_idx = train_test_split(temp_idx, test_size=0.5, stratify=[labels[i] for i in temp_idx])\n",
    "    \n",
    "    train_loader = create_balanced_loader(ds, train_idx, BATCH_SIZE, NUM_WORKERS)\n",
    "    val_loader = DataLoader(Subset(ds, val_idx), batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "    test_loader = DataLoader(Subset(ds, test_idx), batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "    model = create_binary_model()\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "    best_acc = 0.0\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer)\n",
    "        val_metrics = eval_epoch(model, val_loader, criterion)\n",
    "        val_loss = val_metrics[\"loss\"]\n",
    "        val_acc = val_metrics[\"acc\"]\n",
    "        binary_histories[cls].append(val_acc)\n",
    "        save_binary_histories_to_csv(binary_histories)\n",
    "        print(f\"[{cls}] Epoch {epoch}: Val Acc = {val_acc:.4f}\")\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "    ensemble_models[cls] = model\n",
    "    print(f\"\\n--- Evaluation for {cls} ---\")\n",
    "\n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "    train_metrics = eval_epoch(model, train_loader, criterion)\n",
    "    val_metrics = eval_epoch(model, val_loader, criterion)\n",
    "    test_metrics = eval_epoch(model, test_loader, criterion)\n",
    "\n",
    "    def print_metrics(split, metrics):\n",
    "        print(f\"{split} Accuracy: {metrics['acc']:.4f}\")\n",
    "        print(f\"{split} Precision: {metrics['precision']:.4f}\")\n",
    "        print(f\"{split} Recall: {metrics['recall']:.4f}\")\n",
    "        print(f\"{split} F1 Score: {metrics['f1']:.4f}\")\n",
    "        print(f\"{split} MSE: {metrics['mse']:.4f}\")\n",
    "        print(f\"{split} Errors: {metrics['errors']}\")\n",
    "        print(f\"{split} Confusion Matrix:\\n{metrics['confusion_matrix']}\\n\")\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=metrics['confusion_matrix'], display_labels=[\"Other\", cls])\n",
    "        disp.plot(cmap='Blues')\n",
    "        plt.title(f\"{split} Confusion Matrix for {cls}\")\n",
    "        plt.show()\n",
    "\n",
    "    print_metrics(\"Train\", train_metrics)\n",
    "    print_metrics(\"Validation\", val_metrics)\n",
    "    print_metrics(\"Test\", test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58ed7a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Astrocitoma vs Rest ---\n",
      "\n",
      "--- Evaluation for Astrocitoma ---\n",
      "Train Accuracy: 0.5803\n",
      "Train Precision: 0.9558\n",
      "Train Recall: 0.2043\n",
      "Train F1 Score: 0.3366\n",
      "Train MSE: 0.2719\n",
      "Train Errors: 1277\n",
      "Train Confusion Matrix:\n",
      "[[1442   15]\n",
      " [1262  324]]\n",
      "\n",
      "Validation Accuracy: 0.8880\n",
      "Validation Precision: 0.8095\n",
      "Validation Recall: 0.1977\n",
      "Validation F1 Score: 0.3178\n",
      "Validation MSE: 0.0846\n",
      "Validation Errors: 73\n",
      "Validation Confusion Matrix:\n",
      "[[562   4]\n",
      " [ 69  17]]\n",
      "\n",
      "Test Accuracy: 0.8851\n",
      "Test Precision: 0.7037\n",
      "Test Recall: 0.2209\n",
      "Test F1 Score: 0.3363\n",
      "Test MSE: 0.0838\n",
      "Test Errors: 75\n",
      "Test Confusion Matrix:\n",
      "[[559   8]\n",
      " [ 67  19]]\n",
      "\n",
      "\n",
      "--- Training Carcinoma vs Rest ---\n",
      "\n",
      "--- Evaluation for Carcinoma ---\n",
      "Train Accuracy: 0.9510\n",
      "Train Precision: 0.9972\n",
      "Train Recall: 0.9062\n",
      "Train F1 Score: 0.9495\n",
      "Train MSE: 0.0438\n",
      "Train Errors: 149\n",
      "Train Confusion Matrix:\n",
      "[[1493    4]\n",
      " [ 145 1401]]\n",
      "\n",
      "Validation Accuracy: 0.9908\n",
      "Validation Precision: 0.9583\n",
      "Validation Recall: 0.8214\n",
      "Validation F1 Score: 0.8846\n",
      "Validation MSE: 0.0100\n",
      "Validation Errors: 6\n",
      "Validation Confusion Matrix:\n",
      "[[623   1]\n",
      " [  5  23]]\n",
      "\n",
      "Test Accuracy: 0.9939\n",
      "Test Precision: 0.9286\n",
      "Test Recall: 0.9286\n",
      "Test F1 Score: 0.9286\n",
      "Test MSE: 0.0072\n",
      "Test Errors: 4\n",
      "Test Confusion Matrix:\n",
      "[[623   2]\n",
      " [  2  26]]\n",
      "\n",
      "\n",
      "--- Training Ependimoma vs Rest ---\n",
      "\n",
      "--- Evaluation for Ependimoma ---\n",
      "Train Accuracy: 0.7644\n",
      "Train Precision: 1.0000\n",
      "Train Recall: 0.5220\n",
      "Train F1 Score: 0.6859\n",
      "Train MSE: 0.1547\n",
      "Train Errors: 717\n",
      "Train Confusion Matrix:\n",
      "[[1543    0]\n",
      " [ 717  783]]\n",
      "\n",
      "Validation Accuracy: 0.9847\n",
      "Validation Precision: 0.9286\n",
      "Validation Recall: 0.5909\n",
      "Validation F1 Score: 0.7222\n",
      "Validation MSE: 0.0134\n",
      "Validation Errors: 10\n",
      "Validation Confusion Matrix:\n",
      "[[629   1]\n",
      " [  9  13]]\n",
      "\n",
      "Test Accuracy: 0.9816\n",
      "Test Precision: 0.9231\n",
      "Test Recall: 0.5217\n",
      "Test F1 Score: 0.6667\n",
      "Test MSE: 0.0120\n",
      "Test Errors: 12\n",
      "Test Confusion Matrix:\n",
      "[[629   1]\n",
      " [ 11  12]]\n",
      "\n",
      "\n",
      "--- Training Ganglioglioma vs Rest ---\n",
      "\n",
      "--- Evaluation for Ganglioglioma ---\n",
      "Train Accuracy: 0.9517\n",
      "Train Precision: 1.0000\n",
      "Train Recall: 0.9050\n",
      "Train F1 Score: 0.9501\n",
      "Train MSE: 0.0441\n",
      "Train Errors: 147\n",
      "Train Confusion Matrix:\n",
      "[[1496    0]\n",
      " [ 147 1400]]\n",
      "\n",
      "Validation Accuracy: 0.9969\n",
      "Validation Precision: 1.0000\n",
      "Validation Recall: 0.7778\n",
      "Validation F1 Score: 0.8750\n",
      "Validation MSE: 0.0021\n",
      "Validation Errors: 2\n",
      "Validation Confusion Matrix:\n",
      "[[643   0]\n",
      " [  2   7]]\n",
      "\n",
      "Test Accuracy: 0.9985\n",
      "Test Precision: 1.0000\n",
      "Test Recall: 0.8889\n",
      "Test F1 Score: 0.9412\n",
      "Test MSE: 0.0016\n",
      "Test Errors: 1\n",
      "Test Confusion Matrix:\n",
      "[[644   0]\n",
      " [  1   8]]\n",
      "\n",
      "\n",
      "--- Training Germinoma vs Rest ---\n",
      "\n",
      "--- Evaluation for Germinoma ---\n",
      "Train Accuracy: 0.9911\n",
      "Train Precision: 0.9960\n",
      "Train Recall: 0.9861\n",
      "Train F1 Score: 0.9910\n",
      "Train MSE: 0.0054\n",
      "Train Errors: 27\n",
      "Train Confusion Matrix:\n",
      "[[1523    6]\n",
      " [  21 1493]]\n",
      "\n",
      "Validation Accuracy: 1.0000\n",
      "Validation Precision: 1.0000\n",
      "Validation Recall: 1.0000\n",
      "Validation F1 Score: 1.0000\n",
      "Validation MSE: 0.0014\n",
      "Validation Errors: 0\n",
      "Validation Confusion Matrix:\n",
      "[[638   0]\n",
      " [  0  14]]\n",
      "\n",
      "Test Accuracy: 0.9969\n",
      "Test Precision: 0.8824\n",
      "Test Recall: 1.0000\n",
      "Test F1 Score: 0.9375\n",
      "Test MSE: 0.0019\n",
      "Test Errors: 2\n",
      "Test Confusion Matrix:\n",
      "[[636   2]\n",
      " [  0  15]]\n",
      "\n",
      "\n",
      "--- Training Glioblastoma vs Rest ---\n",
      "\n",
      "--- Evaluation for Glioblastoma ---\n",
      "Train Accuracy: 0.9063\n",
      "Train Precision: 0.9943\n",
      "Train Recall: 0.8153\n",
      "Train F1 Score: 0.8959\n",
      "Train MSE: 0.0691\n",
      "Train Errors: 285\n",
      "Train Confusion Matrix:\n",
      "[[1531    7]\n",
      " [ 278 1227]]\n",
      "\n",
      "Validation Accuracy: 0.9893\n",
      "Validation Precision: 0.8929\n",
      "Validation Recall: 0.8621\n",
      "Validation F1 Score: 0.8772\n",
      "Validation MSE: 0.0094\n",
      "Validation Errors: 7\n",
      "Validation Confusion Matrix:\n",
      "[[620   3]\n",
      " [  4  25]]\n",
      "\n",
      "Test Accuracy: 0.9939\n",
      "Test Precision: 0.9643\n",
      "Test Recall: 0.9000\n",
      "Test F1 Score: 0.9310\n",
      "Test MSE: 0.0049\n",
      "Test Errors: 4\n",
      "Test Confusion Matrix:\n",
      "[[622   1]\n",
      " [  3  27]]\n",
      "\n",
      "\n",
      "--- Training Granuloma vs Rest ---\n",
      "\n",
      "--- Evaluation for Granuloma ---\n",
      "Train Accuracy: 0.5925\n",
      "Train Precision: 1.0000\n",
      "Train Recall: 0.2077\n",
      "Train F1 Score: 0.3439\n",
      "Train MSE: 0.3629\n",
      "Train Errors: 1240\n",
      "Train Confusion Matrix:\n",
      "[[1478    0]\n",
      " [1240  325]]\n",
      "\n",
      "Validation Accuracy: 0.9847\n",
      "Validation Precision: 1.0000\n",
      "Validation Recall: 0.0909\n",
      "Validation F1 Score: 0.1667\n",
      "Validation MSE: 0.0142\n",
      "Validation Errors: 10\n",
      "Validation Confusion Matrix:\n",
      "[[641   0]\n",
      " [ 10   1]]\n",
      "\n",
      "Test Accuracy: 0.9847\n",
      "Test Precision: 1.0000\n",
      "Test Recall: 0.1667\n",
      "Test F1 Score: 0.2857\n",
      "Test MSE: 0.0142\n",
      "Test Errors: 10\n",
      "Test Confusion Matrix:\n",
      "[[641   0]\n",
      " [ 10   2]]\n",
      "\n",
      "\n",
      "--- Training Meduloblastoma vs Rest ---\n",
      "\n",
      "--- Evaluation for Meduloblastoma ---\n",
      "Train Accuracy: 0.9362\n",
      "Train Precision: 0.9935\n",
      "Train Recall: 0.8811\n",
      "Train F1 Score: 0.9339\n",
      "Train MSE: 0.0414\n",
      "Train Errors: 194\n",
      "Train Confusion Matrix:\n",
      "[[1478    9]\n",
      " [ 185 1371]]\n",
      "\n",
      "Validation Accuracy: 0.9877\n",
      "Validation Precision: 0.8667\n",
      "Validation Recall: 0.6842\n",
      "Validation F1 Score: 0.7647\n",
      "Validation MSE: 0.0078\n",
      "Validation Errors: 8\n",
      "Validation Confusion Matrix:\n",
      "[[631   2]\n",
      " [  6  13]]\n",
      "\n",
      "Test Accuracy: 0.9923\n",
      "Test Precision: 0.8182\n",
      "Test Recall: 0.9474\n",
      "Test F1 Score: 0.8780\n",
      "Test MSE: 0.0068\n",
      "Test Errors: 5\n",
      "Test Confusion Matrix:\n",
      "[[630   4]\n",
      " [  1  18]]\n",
      "\n",
      "\n",
      "--- Training Meningioma vs Rest ---\n",
      "\n",
      "--- Evaluation for Meningioma ---\n",
      "Train Accuracy: 0.8505\n",
      "Train Precision: 0.9382\n",
      "Train Recall: 0.7614\n",
      "Train F1 Score: 0.8406\n",
      "Train MSE: 0.1065\n",
      "Train Errors: 455\n",
      "Train Confusion Matrix:\n",
      "[[1388   79]\n",
      " [ 376 1200]]\n",
      "\n",
      "Validation Accuracy: 0.8972\n",
      "Validation Precision: 0.7348\n",
      "Validation Recall: 0.7519\n",
      "Validation F1 Score: 0.7433\n",
      "Validation MSE: 0.0770\n",
      "Validation Errors: 67\n",
      "Validation Confusion Matrix:\n",
      "[[488  35]\n",
      " [ 32  97]]\n",
      "\n",
      "Test Accuracy: 0.9219\n",
      "Test Precision: 0.8015\n",
      "Test Recall: 0.8077\n",
      "Test F1 Score: 0.8046\n",
      "Test MSE: 0.0686\n",
      "Test Errors: 51\n",
      "Test Confusion Matrix:\n",
      "[[497  26]\n",
      " [ 25 105]]\n",
      "\n",
      "\n",
      "--- Training NORMAL vs Rest ---\n",
      "\n",
      "--- Evaluation for NORMAL ---\n",
      "Train Accuracy: 0.9333\n",
      "Train Precision: 0.9317\n",
      "Train Recall: 0.9285\n",
      "Train F1 Score: 0.9301\n",
      "Train MSE: 0.0511\n",
      "Train Errors: 203\n",
      "Train Confusion Matrix:\n",
      "[[1489   99]\n",
      " [ 104 1351]]\n",
      "\n",
      "Validation Accuracy: 0.9402\n",
      "Validation Precision: 0.6857\n",
      "Validation Recall: 0.9231\n",
      "Validation F1 Score: 0.7869\n",
      "Validation MSE: 0.0419\n",
      "Validation Errors: 39\n",
      "Validation Confusion Matrix:\n",
      "[[541  33]\n",
      " [  6  72]]\n",
      "\n",
      "Test Accuracy: 0.9556\n",
      "Test Precision: 0.7451\n",
      "Test Recall: 0.9620\n",
      "Test F1 Score: 0.8398\n",
      "Test MSE: 0.0378\n",
      "Test Errors: 29\n",
      "Test Confusion Matrix:\n",
      "[[548  26]\n",
      " [  3  76]]\n",
      "\n",
      "\n",
      "--- Training Neurocitoma vs Rest ---\n",
      "\n",
      "--- Evaluation for Neurocitoma ---\n",
      "Train Accuracy: 0.9261\n",
      "Train Precision: 0.9607\n",
      "Train Recall: 0.8854\n",
      "Train F1 Score: 0.9215\n",
      "Train MSE: 0.0647\n",
      "Train Errors: 225\n",
      "Train Confusion Matrix:\n",
      "[[1497   54]\n",
      " [ 171 1321]]\n",
      "\n",
      "Validation Accuracy: 0.9463\n",
      "Validation Precision: 0.7089\n",
      "Validation Recall: 0.8235\n",
      "Validation F1 Score: 0.7619\n",
      "Validation MSE: 0.0415\n",
      "Validation Errors: 35\n",
      "Validation Confusion Matrix:\n",
      "[[561  23]\n",
      " [ 12  56]]\n",
      "\n",
      "Test Accuracy: 0.9510\n",
      "Test Precision: 0.7534\n",
      "Test Recall: 0.7971\n",
      "Test F1 Score: 0.7746\n",
      "Test MSE: 0.0355\n",
      "Test Errors: 32\n",
      "Test Confusion Matrix:\n",
      "[[566  18]\n",
      " [ 14  55]]\n",
      "\n",
      "\n",
      "--- Training Oligodendroglioma vs Rest ---\n",
      "\n",
      "--- Evaluation for Oligodendroglioma ---\n",
      "Train Accuracy: 0.6464\n",
      "Train Precision: 1.0000\n",
      "Train Recall: 0.2893\n",
      "Train F1 Score: 0.4488\n",
      "Train MSE: 0.2570\n",
      "Train Errors: 1076\n",
      "Train Confusion Matrix:\n",
      "[[1529    0]\n",
      " [1076  438]]\n",
      "\n",
      "Validation Accuracy: 0.9601\n",
      "Validation Precision: 1.0000\n",
      "Validation Recall: 0.2121\n",
      "Validation F1 Score: 0.3500\n",
      "Validation MSE: 0.0305\n",
      "Validation Errors: 26\n",
      "Validation Confusion Matrix:\n",
      "[[619   0]\n",
      " [ 26   7]]\n",
      "\n",
      "Test Accuracy: 0.9541\n",
      "Test Precision: 0.7143\n",
      "Test Recall: 0.1515\n",
      "Test F1 Score: 0.2500\n",
      "Test MSE: 0.0333\n",
      "Test Errors: 30\n",
      "Test Confusion Matrix:\n",
      "[[618   2]\n",
      " [ 28   5]]\n",
      "\n",
      "\n",
      "--- Training Papiloma vs Rest ---\n",
      "\n",
      "--- Evaluation for Papiloma ---\n",
      "Train Accuracy: 0.8068\n",
      "Train Precision: 0.9989\n",
      "Train Recall: 0.6133\n",
      "Train F1 Score: 0.7600\n",
      "Train MSE: 0.1317\n",
      "Train Errors: 588\n",
      "Train Confusion Matrix:\n",
      "[[1524    1]\n",
      " [ 587  931]]\n",
      "\n",
      "Validation Accuracy: 0.9785\n",
      "Validation Precision: 0.9545\n",
      "Validation Recall: 0.6176\n",
      "Validation F1 Score: 0.7500\n",
      "Validation MSE: 0.0173\n",
      "Validation Errors: 14\n",
      "Validation Confusion Matrix:\n",
      "[[617   1]\n",
      " [ 13  21]]\n",
      "\n",
      "Test Accuracy: 0.9678\n",
      "Test Precision: 1.0000\n",
      "Test Recall: 0.3824\n",
      "Test F1 Score: 0.5532\n",
      "Test MSE: 0.0235\n",
      "Test Errors: 21\n",
      "Test Confusion Matrix:\n",
      "[[619   0]\n",
      " [ 21  13]]\n",
      "\n",
      "\n",
      "--- Training Schwannoma vs Rest ---\n",
      "\n",
      "--- Evaluation for Schwannoma ---\n",
      "Train Accuracy: 0.9037\n",
      "Train Precision: 0.9779\n",
      "Train Recall: 0.8236\n",
      "Train F1 Score: 0.8941\n",
      "Train MSE: 0.0685\n",
      "Train Errors: 293\n",
      "Train Confusion Matrix:\n",
      "[[1513   28]\n",
      " [ 265 1237]]\n",
      "\n",
      "Validation Accuracy: 0.9739\n",
      "Validation Precision: 0.8806\n",
      "Validation Recall: 0.8676\n",
      "Validation F1 Score: 0.8741\n",
      "Validation MSE: 0.0247\n",
      "Validation Errors: 17\n",
      "Validation Confusion Matrix:\n",
      "[[576   8]\n",
      " [  9  59]]\n",
      "\n",
      "Test Accuracy: 0.9663\n",
      "Test Precision: 0.8594\n",
      "Test Recall: 0.8088\n",
      "Test F1 Score: 0.8333\n",
      "Test MSE: 0.0257\n",
      "Test Errors: 22\n",
      "Test Confusion Matrix:\n",
      "[[576   9]\n",
      " [ 13  55]]\n",
      "\n",
      "\n",
      "--- Training Tuberculoma vs Rest ---\n",
      "\n",
      "--- Evaluation for Tuberculoma ---\n",
      "Train Accuracy: 0.8577\n",
      "Train Precision: 0.9819\n",
      "Train Recall: 0.7243\n",
      "Train F1 Score: 0.8337\n",
      "Train MSE: 0.1205\n",
      "Train Errors: 433\n",
      "Train Confusion Matrix:\n",
      "[[1525   20]\n",
      " [ 413 1085]]\n",
      "\n",
      "Validation Accuracy: 0.9755\n",
      "Validation Precision: 0.5833\n",
      "Validation Recall: 0.7000\n",
      "Validation F1 Score: 0.6364\n",
      "Validation MSE: 0.0196\n",
      "Validation Errors: 16\n",
      "Validation Confusion Matrix:\n",
      "[[622  10]\n",
      " [  6  14]]\n",
      "\n",
      "Test Accuracy: 0.9694\n",
      "Test Precision: 0.5294\n",
      "Test Recall: 0.4286\n",
      "Test F1 Score: 0.4737\n",
      "Test MSE: 0.0234\n",
      "Test Errors: 20\n",
      "Test Confusion Matrix:\n",
      "[[624   8]\n",
      " [ 12   9]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for cls in class_names:\n",
    "    model_path = MODEL_SAVE_DIR / f\"best_binary_{cls}.pth\"\n",
    "    print(f\"\\n--- Training {cls} vs Rest ---\")\n",
    "    ds = BinaryFolderDataset(DATA_DIR, cls, transform=transform)\n",
    "    labels = [label for _, label in ds.samples]\n",
    "    \n",
    "    train_idx, temp_idx = train_test_split(range(len(ds)), test_size=0.3, stratify=labels)\n",
    "    val_idx, test_idx = train_test_split(temp_idx, test_size=0.5, stratify=[labels[i] for i in temp_idx])\n",
    "    \n",
    "    train_loader = create_balanced_loader(ds, train_idx, BATCH_SIZE, NUM_WORKERS)\n",
    "    val_loader = DataLoader(Subset(ds, val_idx), batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "    test_loader = DataLoader(Subset(ds, test_idx), batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "    model = create_binary_model()\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "    best_acc = 0.0\n",
    "    # for epoch in range(1, EPOCHS + 1):\n",
    "    #     train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer)\n",
    "    #     val_metrics = eval_epoch(model, val_loader, criterion)\n",
    "    #     val_loss = val_metrics[\"loss\"]\n",
    "    #     val_acc = val_metrics[\"acc\"]\n",
    "    #     binary_histories[cls].append(val_acc)\n",
    "    #     save_binary_histories_to_csv(binary_histories)\n",
    "    #     print(f\"[{cls}] Epoch {epoch}: Val Acc = {val_acc:.4f}\")\n",
    "    #     if val_acc > best_acc:\n",
    "    #         best_acc = val_acc\n",
    "    #         torch.save(model.state_dict(), model_path)\n",
    "    ensemble_models[cls] = model\n",
    "    print(f\"\\n--- Evaluation for {cls} ---\")\n",
    "\n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "    train_metrics = eval_epoch(model, train_loader, criterion)\n",
    "    val_metrics = eval_epoch(model, val_loader, criterion)\n",
    "    test_metrics = eval_epoch(model, test_loader, criterion)\n",
    "\n",
    "    def print_metrics(split, metrics):\n",
    "        print(f\"{split} Accuracy: {metrics['acc']:.4f}\")\n",
    "        print(f\"{split} Precision: {metrics['precision']:.4f}\")\n",
    "        print(f\"{split} Recall: {metrics['recall']:.4f}\")\n",
    "        print(f\"{split} F1 Score: {metrics['f1']:.4f}\")\n",
    "        print(f\"{split} MSE: {metrics['mse']:.4f}\")\n",
    "        print(f\"{split} Errors: {metrics['errors']}\")\n",
    "        print(f\"{split} Confusion Matrix:\\n{metrics['confusion_matrix']}\\n\")\n",
    "\n",
    "    print_metrics(\"Train\", train_metrics)\n",
    "    print_metrics(\"Validation\", val_metrics)\n",
    "    print_metrics(\"Test\", test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7b576b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Average Metrics Across All Binary Models ===\n",
      "Average ACC: 0.9694\n",
      "Average PRECISION: 0.5294\n",
      "Average RECALL: 0.4286\n",
      "Average F1: 0.4737\n",
      "Average MSE: 0.0234\n",
      "Average ERRORS: 20.0000\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "avg_metrics = defaultdict(list)  # To collect metrics for averaging\n",
    "for key in [\"acc\", \"precision\", \"recall\", \"f1\", \"mse\", \"errors\"]:\n",
    "    avg_metrics[key].append(test_metrics[key])\n",
    "print(\"\\n=== Average Metrics Across All Binary Models ===\")\n",
    "for key in avg_metrics:\n",
    "    value = np.mean(avg_metrics[key])\n",
    "    print(f\"Average {key.upper()}: {value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e209f841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters: 138,357,544\n",
      "Total trainable parameters: 106,512\n",
      "Total trainable parameters: 13,649,388\n",
      "Total trainable parameters: 106,512\n",
      "Total trainable parameters: 5,182,508\n",
      "Total trainable parameters: 106,512\n",
      "1298.9855039807721\n",
      "128.1488283010365\n",
      "48.6565645185519\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class LightTumorCNN_v2(nn.Module):\n",
    "    def __init__(self, num_classes=1):\n",
    "        super().__init__()\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding=1, stride=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.blocks = nn.Sequential(\n",
    "            DepthwiseSeparableConv(16, 32), nn.MaxPool2d(2),  # 112x112\n",
    "            DepthwiseSeparableConv(32, 64), nn.MaxPool2d(2),  # 56x56\n",
    "            DepthwiseSeparableConv(64, 128), SEBlock(128), nn.MaxPool2d(2),  # 28x28\n",
    "            DepthwiseSeparableConv(128, 128), nn.MaxPool2d(2)  # 14x14\n",
    "        )\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.blocks(x)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "class DepthwiseSeparableConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1, groups=in_channels)\n",
    "        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.depthwise(x)\n",
    "        x = self.pointwise(x)\n",
    "        x = self.bn(x)\n",
    "        return self.relu(x)\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super().__init__()\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channels, channels // reduction),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(channels // reduction, channels),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        se = self.pool(x).view(b, c)\n",
    "        se = self.fc(se).view(b, c, 1, 1)\n",
    "        return x * se\n",
    "def count_trainable_params(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total trainable parameters: {total_params:,}\")\n",
    "    return total_params\n",
    "import timm\n",
    "model1 = timm.create_model('ghostnet_100', pretrained=False, num_classes=1000)\n",
    "model2 = LightTumorCNN_v2(num_classes=1000)\n",
    "model3 = timm.create_model('efficientnetv2_rw_t', pretrained=False, num_classes=1000)  \n",
    "model4 = timm.create_model('vgg16', pretrained=False, num_classes=1000) \n",
    "x = count_trainable_params(model4)/count_trainable_params(model2)\n",
    "x2 = count_trainable_params(model3)/count_trainable_params(model2)\n",
    "x3 = count_trainable_params(model1)/count_trainable_params(model2)\n",
    "print(x)\n",
    "print(x2)\n",
    "print(x3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e675624c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found classes: ['Astrocitoma', 'Carcinoma', 'Ependimoma', 'Ganglioglioma', 'Germinoma', 'Glioblastoma', 'Granuloma', 'Meduloblastoma', 'Meningioma', 'NORMAL', 'Neurocitoma', 'Oligodendroglioma', 'Papiloma', 'Schwannoma', 'Tuberculoma']\n",
      "Model for Astrocitoma already exists. Skipping...\n",
      "Model for Carcinoma already exists. Skipping...\n",
      "Model for Ependimoma already exists. Skipping...\n",
      "Model for Ganglioglioma already exists. Skipping...\n",
      "Model for Germinoma already exists. Skipping...\n",
      "Model for Glioblastoma already exists. Skipping...\n",
      "Model for Granuloma already exists. Skipping...\n",
      "Model for Meduloblastoma already exists. Skipping...\n",
      "Model for Meningioma already exists. Skipping...\n",
      "Model for NORMAL already exists. Skipping...\n",
      "Model for Neurocitoma already exists. Skipping...\n",
      "Model for Oligodendroglioma already exists. Skipping...\n",
      "Model for Papiloma already exists. Skipping...\n",
      "Model for Schwannoma already exists. Skipping...\n",
      "Model for Tuberculoma already exists. Skipping...\n",
      "\n",
      "--- Evaluation for Astrocitoma ---\n",
      "Train Accuracy: 0.6750\n",
      "Train Precision: 0.9088\n",
      "Train Recall: 0.3787\n",
      "Train F1 Score: 0.5346\n",
      "Train MSE: 0.2250\n",
      "Train Errors: 989\n",
      "Train Confusion Matrix:\n",
      "[[1486   57]\n",
      " [ 932  568]]\n",
      "\n",
      "Validation Accuracy: 0.9018\n",
      "Validation Precision: 0.7292\n",
      "Validation Recall: 0.4070\n",
      "Validation F1 Score: 0.5224\n",
      "Validation MSE: 0.0846\n",
      "Validation Errors: 64\n",
      "Validation Confusion Matrix:\n",
      "[[553  13]\n",
      " [ 51  35]]\n",
      "\n",
      "Test Accuracy: 0.8959\n",
      "Test Precision: 0.6667\n",
      "Test Recall: 0.4186\n",
      "Test F1 Score: 0.5143\n",
      "Test MSE: 0.0866\n",
      "Test Errors: 68\n",
      "Test Confusion Matrix:\n",
      "[[549  18]\n",
      " [ 50  36]]\n",
      "\n",
      "\n",
      "--- Evaluation for Carcinoma ---\n",
      "Train Accuracy: 0.9770\n",
      "Train Precision: 0.9851\n",
      "Train Recall: 0.9680\n",
      "Train F1 Score: 0.9764\n",
      "Train MSE: 0.0252\n",
      "Train Errors: 70\n",
      "Train Confusion Matrix:\n",
      "[[1522   22]\n",
      " [  48 1451]]\n",
      "\n",
      "Validation Accuracy: 0.9801\n",
      "Validation Precision: 0.7027\n",
      "Validation Recall: 0.9286\n",
      "Validation F1 Score: 0.8000\n",
      "Validation MSE: 0.0175\n",
      "Validation Errors: 13\n",
      "Validation Confusion Matrix:\n",
      "[[613  11]\n",
      " [  2  26]]\n",
      "\n",
      "Test Accuracy: 0.9801\n",
      "Test Precision: 0.6923\n",
      "Test Recall: 0.9643\n",
      "Test F1 Score: 0.8060\n",
      "Test MSE: 0.0170\n",
      "Test Errors: 13\n",
      "Test Confusion Matrix:\n",
      "[[613  12]\n",
      " [  1  27]]\n",
      "\n",
      "\n",
      "--- Evaluation for Ependimoma ---\n",
      "Train Accuracy: 0.7825\n",
      "Train Precision: 0.9952\n",
      "Train Recall: 0.5590\n",
      "Train F1 Score: 0.7159\n",
      "Train MSE: 0.1718\n",
      "Train Errors: 662\n",
      "Train Confusion Matrix:\n",
      "[[1547    4]\n",
      " [ 658  834]]\n",
      "\n",
      "Validation Accuracy: 0.9847\n",
      "Validation Precision: 1.0000\n",
      "Validation Recall: 0.5455\n",
      "Validation F1 Score: 0.7059\n",
      "Validation MSE: 0.0140\n",
      "Validation Errors: 10\n",
      "Validation Confusion Matrix:\n",
      "[[630   0]\n",
      " [ 10  12]]\n",
      "\n",
      "Test Accuracy: 0.9801\n",
      "Test Precision: 0.8571\n",
      "Test Recall: 0.5217\n",
      "Test F1 Score: 0.6486\n",
      "Test MSE: 0.0147\n",
      "Test Errors: 13\n",
      "Test Confusion Matrix:\n",
      "[[628   2]\n",
      " [ 11  12]]\n",
      "\n",
      "\n",
      "--- Evaluation for Ganglioglioma ---\n",
      "Train Accuracy: 0.9754\n",
      "Train Precision: 0.9986\n",
      "Train Recall: 0.9518\n",
      "Train F1 Score: 0.9746\n",
      "Train MSE: 0.0206\n",
      "Train Errors: 75\n",
      "Train Confusion Matrix:\n",
      "[[1528    2]\n",
      " [  73 1440]]\n",
      "\n",
      "Validation Accuracy: 0.9985\n",
      "Validation Precision: 0.9000\n",
      "Validation Recall: 1.0000\n",
      "Validation F1 Score: 0.9474\n",
      "Validation MSE: 0.0011\n",
      "Validation Errors: 1\n",
      "Validation Confusion Matrix:\n",
      "[[642   1]\n",
      " [  0   9]]\n",
      "\n",
      "Test Accuracy: 0.9923\n",
      "Test Precision: 0.7000\n",
      "Test Recall: 0.7778\n",
      "Test F1 Score: 0.7368\n",
      "Test MSE: 0.0043\n",
      "Test Errors: 5\n",
      "Test Confusion Matrix:\n",
      "[[641   3]\n",
      " [  2   7]]\n",
      "\n",
      "\n",
      "--- Evaluation for Germinoma ---\n",
      "Train Accuracy: 0.8087\n",
      "Train Precision: 0.9979\n",
      "Train Recall: 0.6192\n",
      "Train F1 Score: 0.7642\n",
      "Train MSE: 0.1569\n",
      "Train Errors: 582\n",
      "Train Confusion Matrix:\n",
      "[[1518    2]\n",
      " [ 580  943]]\n",
      "\n",
      "Validation Accuracy: 0.9954\n",
      "Validation Precision: 1.0000\n",
      "Validation Recall: 0.7857\n",
      "Validation F1 Score: 0.8800\n",
      "Validation MSE: 0.0047\n",
      "Validation Errors: 3\n",
      "Validation Confusion Matrix:\n",
      "[[638   0]\n",
      " [  3  11]]\n",
      "\n",
      "Test Accuracy: 0.9908\n",
      "Test Precision: 0.9091\n",
      "Test Recall: 0.6667\n",
      "Test F1 Score: 0.7692\n",
      "Test MSE: 0.0080\n",
      "Test Errors: 6\n",
      "Test Confusion Matrix:\n",
      "[[637   1]\n",
      " [  5  10]]\n",
      "\n",
      "\n",
      "--- Evaluation for Glioblastoma ---\n",
      "Train Accuracy: 0.9685\n",
      "Train Precision: 0.9972\n",
      "Train Recall: 0.9396\n",
      "Train F1 Score: 0.9676\n",
      "Train MSE: 0.0279\n",
      "Train Errors: 96\n",
      "Train Confusion Matrix:\n",
      "[[1515    4]\n",
      " [  92 1432]]\n",
      "\n",
      "Validation Accuracy: 0.9939\n",
      "Validation Precision: 0.9032\n",
      "Validation Recall: 0.9655\n",
      "Validation F1 Score: 0.9333\n",
      "Validation MSE: 0.0079\n",
      "Validation Errors: 4\n",
      "Validation Confusion Matrix:\n",
      "[[620   3]\n",
      " [  1  28]]\n",
      "\n",
      "Test Accuracy: 0.9893\n",
      "Test Precision: 0.8485\n",
      "Test Recall: 0.9333\n",
      "Test F1 Score: 0.8889\n",
      "Test MSE: 0.0099\n",
      "Test Errors: 7\n",
      "Test Confusion Matrix:\n",
      "[[618   5]\n",
      " [  2  28]]\n",
      "\n",
      "\n",
      "--- Evaluation for Granuloma ---\n",
      "Train Accuracy: 0.9192\n",
      "Train Precision: 0.9923\n",
      "Train Recall: 0.8455\n",
      "Train F1 Score: 0.9131\n",
      "Train MSE: 0.0665\n",
      "Train Errors: 246\n",
      "Train Confusion Matrix:\n",
      "[[1505   10]\n",
      " [ 236 1292]]\n",
      "\n",
      "Validation Accuracy: 0.9939\n",
      "Validation Precision: 0.7692\n",
      "Validation Recall: 0.9091\n",
      "Validation F1 Score: 0.8333\n",
      "Validation MSE: 0.0060\n",
      "Validation Errors: 4\n",
      "Validation Confusion Matrix:\n",
      "[[638   3]\n",
      " [  1  10]]\n",
      "\n",
      "Test Accuracy: 0.9939\n",
      "Test Precision: 0.7500\n",
      "Test Recall: 1.0000\n",
      "Test F1 Score: 0.8571\n",
      "Test MSE: 0.0053\n",
      "Test Errors: 4\n",
      "Test Confusion Matrix:\n",
      "[[637   4]\n",
      " [  0  12]]\n",
      "\n",
      "\n",
      "--- Evaluation for Meduloblastoma ---\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, WeightedRandomSampler\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, mean_squared_error, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "# ==== Hyperparameters and Paths ====\n",
    "DATA_DIR = Path(\"./dataset\")\n",
    "MODEL_SAVE_DIR = Path(\"./models_customcnn2\")\n",
    "MODEL_SAVE_DIR.mkdir(exist_ok=True)\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 64\n",
    "LR = 5e-4\n",
    "EPOCHS = 10\n",
    "NUM_WORKERS = 0 if os.name == 'nt' else 4\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# ==== Dataset Setup ====\n",
    "dirs = [d.name for d in DATA_DIR.iterdir() if d.is_dir()]\n",
    "class_names = sorted(dirs)\n",
    "num_classes = len(class_names)\n",
    "print(\"Found classes:\", class_names)\n",
    "\n",
    "class BinaryFolderDataset(Dataset):\n",
    "    def __init__(self, root_dir, positive_class, transform=None):\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "        for cls_dir in Path(root_dir).iterdir():\n",
    "            if not cls_dir.is_dir():\n",
    "                continue\n",
    "            label = 1 if cls_dir.name == positive_class else 0\n",
    "            for ext in (\"*.jpg\", \"*.jpeg\", \"*.png\"):\n",
    "                for img_path in cls_dir.glob(ext):\n",
    "                    self.samples.append((img_path, label))\n",
    "        if not self.samples:\n",
    "            raise RuntimeError(f\"No images found for class '{positive_class}' in {root_dir}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "# ==== Transforms ====\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "# ==== Custom Lightweight CNN ====\n",
    "class LightTumorCNN_v2(nn.Module):\n",
    "    def __init__(self, num_classes=1):\n",
    "        super().__init__()\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding=1, stride=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.blocks = nn.Sequential(\n",
    "            DepthwiseSeparableConv(16, 32), nn.MaxPool2d(2),  # 112x112\n",
    "            DepthwiseSeparableConv(32, 64), nn.MaxPool2d(2),  # 56x56\n",
    "            DepthwiseSeparableConv(64, 128), SEBlock(128), nn.MaxPool2d(2),  # 28x28\n",
    "            DepthwiseSeparableConv(128, 128), nn.MaxPool2d(2)  # 14x14\n",
    "        )\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.blocks(x)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "class DepthwiseSeparableConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1, groups=in_channels)\n",
    "        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.depthwise(x)\n",
    "        x = self.pointwise(x)\n",
    "        x = self.bn(x)\n",
    "        return self.relu(x)\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super().__init__()\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channels, channels // reduction),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(channels // reduction, channels),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        se = self.pool(x).view(b, c)\n",
    "        se = self.fc(se).view(b, c, 1, 1)\n",
    "        return x * se\n",
    "    \n",
    "\n",
    "def create_binary_model():\n",
    "    model = LightTumorCNN_v2(num_classes=1)\n",
    "    return model.to(device)\n",
    "\n",
    "# ==== Training/Evaluation Functions ====\n",
    "def train_epoch(model, loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x)\n",
    "        y = y.unsqueeze(1).float()\n",
    "        loss = criterion(out, y)\n",
    "        preds = (torch.sigmoid(out) > 0.5).int()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * x.size(0)\n",
    "        correct += (preds.view(-1) == y.view(-1)).sum().item()\n",
    "        total += x.size(0)\n",
    "    return running_loss / total, correct / total\n",
    "\n",
    "def eval_epoch(model, loader, criterion):\n",
    "    model.eval()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    y_true, y_pred, y_prob = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            out = model(x)\n",
    "            y = y.unsqueeze(1).float()\n",
    "            loss = criterion(out, y)\n",
    "            probs = torch.sigmoid(out)\n",
    "            preds = (probs > 0.5).int()\n",
    "            running_loss += loss.item() * x.size(0)\n",
    "            correct += (preds.view(-1) == y.view(-1)).sum().item()\n",
    "            total += x.size(0)\n",
    "            y_true.append(y.view(-1).cpu())\n",
    "            y_pred.append(preds.view(-1).cpu())\n",
    "            y_prob.append(probs.view(-1).cpu())\n",
    "    y_true = torch.cat(y_true).numpy()\n",
    "    y_pred = torch.cat(y_pred).numpy()\n",
    "    y_prob = torch.cat(y_prob).numpy()\n",
    "    \n",
    "    acc = correct / total\n",
    "    mse = mean_squared_error(y_true, y_prob)\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    errors = np.sum(y_true != y_pred)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    return {\n",
    "        \"loss\": running_loss / total,\n",
    "        \"acc\": acc,\n",
    "        \"mse\": mse,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"errors\": errors,\n",
    "        \"confusion_matrix\": cm,\n",
    "        \"y_true\": y_true,\n",
    "        \"y_pred\": y_pred\n",
    "    }\n",
    "\n",
    "\n",
    "def create_balanced_loader(dataset, indices, batch_size, num_workers):\n",
    "    targets = [dataset.samples[i][1] for i in indices]\n",
    "    class_sample_count = np.bincount(targets)\n",
    "    weight = 1. / class_sample_count\n",
    "    samples_weight = np.array([weight[t] for t in targets])\n",
    "    sampler = WeightedRandomSampler(samples_weight, num_samples=len(samples_weight), replacement=True)\n",
    "    loader = DataLoader(Subset(dataset, indices), batch_size=batch_size, sampler=sampler, num_workers=num_workers)\n",
    "    return loader\n",
    "\n",
    "def save_binary_histories_to_csv(binary_histories, filename='binary_histories_customcnn.csv'):\n",
    "    with open(filename, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Class'] + [f\"Epoch {i+1}\" for i in range(len(next(iter(binary_histories.values()))))])\n",
    "        for cls, history in binary_histories.items():\n",
    "            writer.writerow([cls] + history)\n",
    "\n",
    "# ==== Training Binary Ensemble Models ====\n",
    "ensemble_models = {}\n",
    "binary_histories = {cls: [] for cls in class_names}\n",
    "\n",
    "for cls in class_names:\n",
    "    model_path = MODEL_SAVE_DIR / f\"best_binary_{cls}.pth\"\n",
    "    if model_path.exists():\n",
    "        print(f\"Model for {cls} already exists. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n--- Training {cls} vs Rest ---\")\n",
    "    ds = BinaryFolderDataset(DATA_DIR, cls, transform=transform)\n",
    "    labels = [label for _, label in ds.samples]\n",
    "    \n",
    "    train_idx, temp_idx = train_test_split(range(len(ds)), test_size=0.3, stratify=labels)\n",
    "    val_idx, test_idx = train_test_split(temp_idx, test_size=0.5, stratify=[labels[i] for i in temp_idx])\n",
    "    \n",
    "    train_loader = create_balanced_loader(ds, train_idx, BATCH_SIZE, NUM_WORKERS)\n",
    "    val_loader = DataLoader(Subset(ds, val_idx), batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "    test_loader = DataLoader(Subset(ds, test_idx), batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "    model = create_binary_model()\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "    best_acc = 0.0\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer)\n",
    "        val_metrics = eval_epoch(model, val_loader, criterion)\n",
    "        val_loss = val_metrics[\"loss\"]\n",
    "        val_acc = val_metrics[\"acc\"]\n",
    "        binary_histories[cls].append(val_acc)\n",
    "        save_binary_histories_to_csv(binary_histories)\n",
    "        print(f\"[{cls}] Epoch {epoch}: Val Acc = {val_acc:.4f}\")\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "    ensemble_models[cls] = model\n",
    "\n",
    "    \n",
    "for cls in class_names:\n",
    "    model_path = MODEL_SAVE_DIR / f\"best_binary_{cls}.pth\"\n",
    "\n",
    "    ds = BinaryFolderDataset(DATA_DIR, cls, transform=transform)\n",
    "    labels = [label for _, label in ds.samples]\n",
    "    \n",
    "    train_idx, temp_idx = train_test_split(range(len(ds)), test_size=0.3, stratify=labels)\n",
    "    val_idx, test_idx = train_test_split(temp_idx, test_size=0.5, stratify=[labels[i] for i in temp_idx])\n",
    "    \n",
    "    train_loader = create_balanced_loader(ds, train_idx, BATCH_SIZE, NUM_WORKERS)\n",
    "    val_loader = DataLoader(Subset(ds, val_idx), batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "    test_loader = DataLoader(Subset(ds, test_idx), batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "    model = create_binary_model()\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "    ensemble_models[cls] = model\n",
    "    print(f\"\\n--- Evaluation for {cls} ---\")\n",
    "\n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "    train_metrics = eval_epoch(model, train_loader, criterion)\n",
    "    val_metrics = eval_epoch(model, val_loader, criterion)\n",
    "    test_metrics = eval_epoch(model, test_loader, criterion)\n",
    "\n",
    "    def print_metrics(split, metrics):\n",
    "        print(f\"{split} Accuracy: {metrics['acc']:.4f}\")\n",
    "        print(f\"{split} Precision: {metrics['precision']:.4f}\")\n",
    "        print(f\"{split} Recall: {metrics['recall']:.4f}\")\n",
    "        print(f\"{split} F1 Score: {metrics['f1']:.4f}\")\n",
    "        print(f\"{split} MSE: {metrics['mse']:.4f}\")\n",
    "        print(f\"{split} Errors: {metrics['errors']}\")\n",
    "        print(f\"{split} Confusion Matrix:\\n{metrics['confusion_matrix']}\\n\")\n",
    "\n",
    "\n",
    "    print_metrics(\"Train\", train_metrics)\n",
    "    print_metrics(\"Validation\", val_metrics)\n",
    "    print_metrics(\"Test\", test_metrics)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9558086",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "avg_metrics = defaultdict(list)  # To collect metrics for averaging\n",
    "for key in [\"acc\", \"precision\", \"recall\", \"f1\", \"mse\", \"errors\"]:\n",
    "    avg_metrics[key].append(test_metrics[key])\n",
    "print(\"\\n=== Average Metrics Across All Binary Models ===\")\n",
    "for key in avg_metrics:\n",
    "    value = np.mean(avg_metrics[key])\n",
    "    print(f\"Average {key.upper()}: {value:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
